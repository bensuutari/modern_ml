{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer,pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer,pipeline\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "class ChatBot(object):\n",
    "    def __init__(self,model_id):\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_id,load_in_4bit=True,)\n",
    "        self.model.eval()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        chat = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful and honest assistant.\"},\n",
    "        ]\n",
    "        print(tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "    def chat(self,prompt):\n",
    "        #prompt = \"Hello, how are you doing today?\"\n",
    "        inputs = self.tokenizer(prompt,return_tensors=\"pt\").input_ids\n",
    "        with torch.no_grad():\n",
    "            out = self.model.generate(inputs,max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n",
    "        #probs = F.softmax(logits['logits'],dim=-1)\n",
    "        #toks = probs.argmax(dim=-1)\n",
    "        return self.tokenizer.batch_decode(out)#self.tokenizer.batch_decode(toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = ChatBot(\"meta-llama/Llama-2-13b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = chatbot.chat(\"Hello, please explain how a catalytic converter works.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = chatbot.tokenizer.batch_decode(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,pipeline\n",
    "class ChatBot(object):\n",
    "    def __init__(self,model_id):\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id,load_in_4bit=True)\n",
    "        model.eval()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.chat_history = [\n",
    "            {\"role\": \"system\", \"content\" : \"You are a helpful and intelligent AI assistant who responds to user queries.\"}\n",
    "        ]\n",
    "        self.pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    def chat(self,prompt):\n",
    "        self.chat_history.append(\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        )\n",
    "        resp = self.pipe(self.chat_history,max_new_tokens=512)\n",
    "        self.chat_history.append(\n",
    "            {\"role\": \"system\", \"content\" : resp[0]['generated_text'][-1]['content']}\n",
    "        )\n",
    "        return self.chat_history[-1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c113cac8b64feb8e061e569465a084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chatbot = ChatBot(\"meta-llama/Llama-2-13b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/modern_ml/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "response = chatbot.chat(\"What is a catalytic converter?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hello! I'd be happy to help you understand what a catalytic converter is.\n",
      "\n",
      "A catalytic converter is an emissions control device that is used in vehicles to reduce the amount of harmful pollutants emitted into the air. It is typically located in the exhaust system of a vehicle and is designed to convert harmful pollutants, such as carbon monoxide (CO), hydrocarbons (HC), and nitrogen oxides (NOx), into less harmful substances.\n",
      "\n",
      "The catalytic converter works by using a catalyst, which is a substance that speeds up a chemical reaction without being consumed by the reaction. The catalyst is typically a precious metal, such as platinum or palladium, which is coated onto a ceramic or metallic honeycomb-like structure inside the converter.\n",
      "\n",
      "As exhaust gas flows through the converter, it comes into contact with the catalyst, which causes a chemical reaction to occur that converts the pollutants into less harmful substances. For example, CO is converted into carbon dioxide (CO2), which is a harmless gas. HC and NOx are converted into water (H2O) and nitrogen (N2), respectively.\n",
      "\n",
      "Overall, the catalytic converter plays a critical role in reducing the emissions of harmful pollutants from vehicles, helping to improve air quality and reduce the negative impacts of vehicle emissions on the environment and human health.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = chatbot.chat(\"Can you explain what you mean by 'catalyst'?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Certainly! In the context of a catalytic converter, a catalyst is a substance that speeds up a chemical reaction without being consumed by the reaction itself. In other words, the catalyst facilitates the reaction but remains unchanged at the end of the process.\n",
      "\n",
      "In the case of a catalytic converter, the catalyst is typically a precious metal, such as platinum or palladium, which is embedded in a ceramic or metallic honeycomb-like structure. As exhaust gas flows through the converter, the catalyst helps to convert harmful pollutants like carbon monoxide (CO), hydrocarbons (HC), and nitrogen oxides (NOx) into less harmful substances like carbon dioxide (CO2), water (H2O), and nitrogen (N2).\n",
      "\n",
      "The catalyst works by providing a surface for the chemical reactions to occur on. The exhaust gas molecules come into contact with the catalyst and undergo a series of chemical reactions that transform them into less harmful substances. The catalyst remains unchanged throughout the process, allowing it to continue facilitating the reaction over and over again.\n",
      "\n",
      "Overall, the use of a catalyst in a catalytic converter is what allows the converter to efficiently and effectively reduce the emissions of harmful pollutants from a vehicle's exhaust.\n"
     ]
    }
   ],
   "source": [
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,load_in_4bit=True)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hey, can you tell me any fun things to do in New York?\"}\n",
    "]\n",
    "\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model,tokenizer=tokenizer, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "response = pipe(chat, max_new_tokens=512)\n",
    "print(response[0]['generated_text'][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(response)\n",
    "type(response[0])\n",
    "response[0].keys()\n",
    "response[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaotic playground:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "#model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model_id = \"SweatyCrayfish/llama-3-8b-quantized\"\n",
    "\n",
    "pipeline = transformers.pipeline(\"text-generation\", model=model_id, load_in_4bit=True, device_map=\"auto\",torch_dtype=torch.float32)\n",
    "pipeline(\"Hey how are you doing today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(\"SweatyCrayfish/llama-3-8b-quantized\", load_in_4bit=True, torch_dtype=torch.float32)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SweatyCrayfish/llama-3-8b-quantized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4bit.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "import torch \n",
    "#model_id = \"Meta-Llama-3-8B.Q2_K.gguf\"\n",
    "model_id = \"QuantFactory/Meta-Llama-3-8B-Instruct-GGUF-v2\"\n",
    "#model_4bit = AutoModelForCausalLM.from_pretrained(\"SweatyCrayfish/llama-3-8b-quantized\", load_in_4bit=True, torch_dtype=torch.float32)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"SweatyCrayfish/llama-3-8b-quantized\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"QuantFactory/Meta-Llama-3-8B-GGUF\")\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"QuantFactory/Meta-Llama-3-8B-Instruct-GGUF-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = torch.tensor(tokenizer.encode(\"Hey how are you doing today?\"))\n",
    "prompt = \"What is the Python programming language?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model_4bit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "\n",
    "out = F.softmax(y['logits'],dim=-1).argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(tokenizer)\n",
    "tokenizer.batch_decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer,pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"gpt2\", load_in_4bit=True, device_map=\"auto\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "#pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hello, how are you doing today?\"\n",
    "inputs = tokenizer(prompt,return_tensors=\"pt\").input_ids\n",
    "out = model.generate(inputs,max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out\n",
    "tokenizer.batch_decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"Hello, how are you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokenizer.default_chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the Python programming language?\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probs = F.softmax(logits['logits'],dim=-1)\n",
    "toks = probs.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.default_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", \"HuggingFaceH4/zephyr-7b-beta\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1])  # Print the assistant's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", \"HuggingFaceH4/zephyr-7b-beta\")\n",
    "pipe = pipeline(\"text-generation\",\"meta-llama/Llama-2-13b-chat-hf\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1])  # Print the assistant's response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modern_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
