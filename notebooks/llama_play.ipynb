{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer,pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer,pipeline\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "class ChatBot(object):\n",
    "    def __init__(self,model_id):\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_id,load_in_4bit=True,)\n",
    "        self.model.eval()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        chat = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful and honest assistant.\"},\n",
    "        ]\n",
    "        print(tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "    def chat(self,prompt):\n",
    "        #prompt = \"Hello, how are you doing today?\"\n",
    "        inputs = self.tokenizer(prompt,return_tensors=\"pt\").input_ids\n",
    "        with torch.no_grad():\n",
    "            out = self.model.generate(inputs,max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n",
    "        #probs = F.softmax(logits['logits'],dim=-1)\n",
    "        #toks = probs.argmax(dim=-1)\n",
    "        return self.tokenizer.batch_decode(out)#self.tokenizer.batch_decode(toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = ChatBot(\"meta-llama/Llama-2-13b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = chatbot.chat(\"Hello, please explain how a catalytic converter works.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = chatbot.tokenizer.batch_decode(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat pipeline\n",
    "\n",
    "Prompting with Llama2: https://discuss.huggingface.co/t/trying-to-understand-system-prompts-with-llama-2-and-transformers-interface/59016\n",
    "\n",
    "Pipelines in Huggingface: https://huggingface.co/docs/transformers/en/add_new_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,pipeline\n",
    "class ChatBot(object):\n",
    "    def __init__(self,model_id):\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id,load_in_4bit=True)\n",
    "        model.eval()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.chat_history = [\n",
    "            {\"role\": \"system\", \"content\" : \"You are a helpful and intelligent AI assistant who responds to user queries.\"}\n",
    "        ]\n",
    "        self.pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.num_tokens = len(self.tokenizer(self.chat_history[0]['content']).input_ids)\n",
    "    def calculate_num_tokens(self,prompt):\n",
    "        self.num_tokens+=len(self.tokenizer(prompt).input_ids)\n",
    "        print('num_tokens:',self.num_tokens)\n",
    "    def chat(self,prompt):\n",
    "        self.calculate_num_tokens(prompt)\n",
    "        self.chat_history.append(\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        )\n",
    "        resp = self.pipe(self.chat_history,max_new_tokens=512)\n",
    "        print(resp)\n",
    "        self.chat_history.append(\n",
    "            {\"role\": \"system\", \"content\" : resp[0]['generated_text'][-1]['content']}\n",
    "        )\n",
    "        self.calculate_num_tokens(self.chat_history[-1][\"content\"])\n",
    "        return self.chat_history[-1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_assisted_decoding',\n",
       " '_auto_class',\n",
       " '_autoset_attn_implementation',\n",
       " '_backward_compatibility_gradient_checkpointing',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_beam_sample',\n",
       " '_beam_search',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_check_and_enable_flash_attn_2',\n",
       " '_check_and_enable_sdpa',\n",
       " '_compiled_call_impl',\n",
       " '_constrained_beam_search',\n",
       " '_contrastive_search',\n",
       " '_convert_head_mask_to_5d',\n",
       " '_copy_lm_head_original_to_resized',\n",
       " '_create_repo',\n",
       " '_dispatch_accelerate_model',\n",
       " '_expand_inputs_for_generation',\n",
       " '_extract_past_from_model_output',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_from_config',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_candidate_generator',\n",
       " '_get_decoder_start_token_id',\n",
       " '_get_files_timestamps',\n",
       " '_get_initial_cache_position',\n",
       " '_get_logits_processor',\n",
       " '_get_logits_warper',\n",
       " '_get_name',\n",
       " '_get_no_split_modules',\n",
       " '_get_resized_embeddings',\n",
       " '_get_resized_lm_head',\n",
       " '_get_static_cache',\n",
       " '_get_stopping_criteria',\n",
       " '_greedy_search',\n",
       " '_group_beam_search',\n",
       " '_has_unfinished_sequences',\n",
       " '_hf_hook',\n",
       " '_hf_peft_config_loaded',\n",
       " '_hook_rss_memory_post_forward',\n",
       " '_hook_rss_memory_pre_forward',\n",
       " '_init_weights',\n",
       " '_initialize_weights',\n",
       " '_is_full_backward_hook',\n",
       " '_is_hf_initialized',\n",
       " '_is_quantized_training_enabled',\n",
       " '_keep_in_fp32_modules',\n",
       " '_keep_in_fp32_modules',\n",
       " '_keys_to_ignore_on_load_missing',\n",
       " '_keys_to_ignore_on_load_unexpected',\n",
       " '_keys_to_ignore_on_save',\n",
       " '_load_from_state_dict',\n",
       " '_load_pretrained_model',\n",
       " '_load_pretrained_model_low_mem',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_initialize_input_ids_for_generation',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_merge_criteria_processor_list',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_no_split_modules',\n",
       " '_non_persistent_buffers_set',\n",
       " '_old_forward',\n",
       " '_parameters',\n",
       " '_prepare_attention_mask_for_generation',\n",
       " '_prepare_decoder_input_ids_for_generation',\n",
       " '_prepare_encoder_decoder_kwargs_for_generation',\n",
       " '_prepare_generated_length',\n",
       " '_prepare_generation_config',\n",
       " '_prepare_model_inputs',\n",
       " '_prepare_special_tokens',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_reorder_cache',\n",
       " '_replicate_for_data_parallel',\n",
       " '_resize_token_embeddings',\n",
       " '_sample',\n",
       " '_save_to_state_dict',\n",
       " '_set_default_torch_dtype',\n",
       " '_set_gradient_checkpointing',\n",
       " '_skip_keys_device_placement',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_supports_cache_class',\n",
       " '_supports_flash_attn_2',\n",
       " '_supports_sdpa',\n",
       " '_supports_static_cache',\n",
       " '_temporary_reorder_cache',\n",
       " '_tie_encoder_decoder_weights',\n",
       " '_tie_or_clone_weights',\n",
       " '_tied_weights_keys',\n",
       " '_update_model_kwargs_for_generation',\n",
       " '_upload_modified_files',\n",
       " '_validate_generated_length',\n",
       " '_validate_model_class',\n",
       " '_validate_model_kwargs',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'active_adapter',\n",
       " 'active_adapters',\n",
       " 'add_adapter',\n",
       " 'add_memory_hooks',\n",
       " 'add_model_tags',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'base_model',\n",
       " 'base_model_prefix',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'can_generate',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'compute_transition_scores',\n",
       " 'config',\n",
       " 'config_class',\n",
       " 'cpu',\n",
       " 'create_extended_attention_mask_for_decoder',\n",
       " 'cuda',\n",
       " 'cuda',\n",
       " 'dequantize',\n",
       " 'device',\n",
       " 'disable_adapters',\n",
       " 'disable_input_require_grads',\n",
       " 'double',\n",
       " 'dtype',\n",
       " 'dummy_inputs',\n",
       " 'dump_patches',\n",
       " 'enable_adapters',\n",
       " 'enable_input_require_grads',\n",
       " 'estimate_tokens',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'floating_point_ops',\n",
       " 'forward',\n",
       " 'forward',\n",
       " 'framework',\n",
       " 'from_pretrained',\n",
       " 'generate',\n",
       " 'generation_config',\n",
       " 'get_adapter_state_dict',\n",
       " 'get_buffer',\n",
       " 'get_decoder',\n",
       " 'get_extended_attention_mask',\n",
       " 'get_extra_state',\n",
       " 'get_head_mask',\n",
       " 'get_input_embeddings',\n",
       " 'get_memory_footprint',\n",
       " 'get_output_embeddings',\n",
       " 'get_parameter',\n",
       " 'get_position_embeddings',\n",
       " 'get_submodule',\n",
       " 'gradient_checkpointing_disable',\n",
       " 'gradient_checkpointing_enable',\n",
       " 'half',\n",
       " 'hf_device_map',\n",
       " 'hf_quantizer',\n",
       " 'init_weights',\n",
       " 'invert_attention_mask',\n",
       " 'ipu',\n",
       " 'is_4bit_serializable',\n",
       " 'is_gradient_checkpointing',\n",
       " 'is_loaded_in_4bit',\n",
       " 'is_parallelizable',\n",
       " 'is_quantized',\n",
       " 'lm_head',\n",
       " 'load_adapter',\n",
       " 'load_state_dict',\n",
       " 'main_input_name',\n",
       " 'model',\n",
       " 'model_tags',\n",
       " 'modules',\n",
       " 'name_or_path',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'num_parameters',\n",
       " 'parameters',\n",
       " 'post_init',\n",
       " 'prepare_inputs_for_generation',\n",
       " 'prune_heads',\n",
       " 'push_to_hub',\n",
       " 'quantization_method',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_for_auto_class',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'reset_memory_hooks_state',\n",
       " 'resize_position_embeddings',\n",
       " 'resize_token_embeddings',\n",
       " 'retrieve_modules_from_names',\n",
       " 'reverse_bettertransformer',\n",
       " 'save_pretrained',\n",
       " 'set_adapter',\n",
       " 'set_decoder',\n",
       " 'set_extra_state',\n",
       " 'set_input_embeddings',\n",
       " 'set_output_embeddings',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'supports_gradient_checkpointing',\n",
       " 'tie_weights',\n",
       " 'to',\n",
       " 'to',\n",
       " 'to_bettertransformer',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'vocab_size',\n",
       " 'warn_if_padding_and_no_attention_mask',\n",
       " 'warnings_issued',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(chatbot.pipe.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c3be8eddf4405a9f955c5b20636587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chatbot = ChatBot(\"meta-llama/Llama-2-13b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_tokens: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/modern_ml/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': [{'role': 'system', 'content': 'You are a helpful and intelligent AI assistant who responds to user queries.'}, {'role': 'user', 'content': 'What is a catalytic converter?'}, {'role': 'assistant', 'content': \"  Hello! I'd be happy to help you understand what a catalytic converter is.\\n\\nA catalytic converter is an emissions control device that is used in vehicles to reduce the amount of harmful pollutants emitted into the atmosphere. It is typically located in the exhaust system of a vehicle and is designed to convert harmful pollutants like carbon monoxide (CO), hydrocarbons (HC), and nitrogen oxides (NOx) into less harmful substances like carbon dioxide (CO2), water (H2O), and nitrogen (N2).\\n\\nThe catalytic converter works by using a catalyst, typically a precious metal like platinum or palladium, to facilitate a chemical reaction that converts the harmful pollutants into less harmful substances. The catalyst is coated onto a ceramic or metallic honeycomb-like structure, which is located within the converter.\\n\\nAs exhaust gas flows through the converter, it comes into contact with the catalyst, which causes a chemical reaction to occur. This reaction converts the harmful pollutants into less harmful substances, which are then released into the atmosphere.\\n\\nOverall, the catalytic converter plays a critical role in reducing the emissions of harmful pollutants from vehicles, helping to improve air quality and reduce the negative impacts of vehicle emissions on the environment and human health.\"}]}]\n",
      "num_tokens: 337\n"
     ]
    }
   ],
   "source": [
    "response = chatbot.chat(\"What is a catalytic converter?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Hello! I'd be happy to help you understand what a catalytic converter is.\n",
      "\n",
      "A catalytic converter is an emissions control device that is used in vehicles to reduce the amount of harmful pollutants emitted into the atmosphere. It is typically located in the exhaust system of a vehicle and is designed to convert harmful pollutants like carbon monoxide (CO), hydrocarbons (HC), and nitrogen oxides (NOx) into less harmful substances like carbon dioxide (CO2), water (H2O), and nitrogen (N2).\n",
      "\n",
      "The catalytic converter works by using a catalyst, typically a precious metal like platinum or palladium, to facilitate a chemical reaction that converts the harmful pollutants into less harmful substances. The catalyst is coated onto a ceramic or metallic honeycomb-like structure, which is located within the converter.\n",
      "\n",
      "As exhaust gas flows through the converter, it comes into contact with the catalyst, which causes a chemical reaction to occur. This reaction converts the harmful pollutants into less harmful substances, which are then released into the atmosphere.\n",
      "\n",
      "Overall, the catalytic converter plays a critical role in reducing the emissions of harmful pollutants from vehicles, helping to improve air quality and reduce the negative impacts of vehicle emissions on the environment and human health.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_tokens: 350\n",
      "[{'generated_text': [{'role': 'system', 'content': 'You are a helpful and intelligent AI assistant who responds to user queries.'}, {'role': 'user', 'content': 'What is a catalytic converter?'}, {'role': 'system', 'content': \"  Hello! I'd be happy to help you understand what a catalytic converter is.\\n\\nA catalytic converter is an emissions control device that is used in vehicles to reduce the amount of harmful pollutants emitted into the atmosphere. It is typically located in the exhaust system of a vehicle and is designed to convert harmful pollutants like carbon monoxide (CO), hydrocarbons (HC), and nitrogen oxides (NOx) into less harmful substances like carbon dioxide (CO2), water (H2O), and nitrogen (N2).\\n\\nThe catalytic converter works by using a catalyst, typically a precious metal like platinum or palladium, to facilitate a chemical reaction that converts the harmful pollutants into less harmful substances. The catalyst is coated onto a ceramic or metallic honeycomb-like structure, which is located within the converter.\\n\\nAs exhaust gas flows through the converter, it comes into contact with the catalyst, which causes a chemical reaction to occur. This reaction converts the harmful pollutants into less harmful substances, which are then released into the atmosphere.\\n\\nOverall, the catalytic converter plays a critical role in reducing the emissions of harmful pollutants from vehicles, helping to improve air quality and reduce the negative impacts of vehicle emissions on the environment and human health.\"}, {'role': 'user', 'content': \"Can you explain what you mean by 'catalyst'?\"}, {'role': 'assistant', 'content': \"  Certainly! In the context of a catalytic converter, a catalyst is a substance that speeds up a chemical reaction without being consumed by the reaction. In other words, the catalyst helps to facilitate the reaction, but it is not itself changed or consumed by the reaction.\\n\\nIn the case of a catalytic converter, the catalyst is typically a precious metal, such as platinum or palladium, which is embedded in a ceramic or metallic honeycomb-like structure. As exhaust gas flows through the converter, the catalyst causes the harmful pollutants in the exhaust, such as carbon monoxide (CO), hydrocarbons (HC), and nitrogen oxides (NOx), to undergo a chemical reaction that transforms them into less harmful substances.\\n\\nThe catalyst works by providing a surface for the pollutants to react with, and by promoting the chemical reactions that convert the pollutants into less harmful substances. The catalyst is not consumed by the reaction, so it can continue to facilitate the conversion of pollutants over time.\\n\\nOverall, the use of a catalyst in a catalytic converter is an important aspect of the technology, as it allows the converter to efficiently and effectively reduce the emissions of harmful pollutants from a vehicle's exhaust.\"}]}]\n",
      "num_tokens: 650\n"
     ]
    }
   ],
   "source": [
    "response2 = chatbot.chat(\"Can you explain what you mean by 'catalyst'?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response3 = chatbot.chat(\"When was the catalytic converter invented?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot.num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response4 = chatbot.chat(\"What other parts of a car are important for pollution control?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot.calculate_num_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,load_in_4bit=True)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hey, can you tell me any fun things to do in New York?\"}\n",
    "]\n",
    "\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model,tokenizer=tokenizer, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "response = pipe(chat, max_new_tokens=512)\n",
    "print(response[0]['generated_text'][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(response)\n",
    "type(response[0])\n",
    "response[0].keys()\n",
    "response[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaotic playground:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "#model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model_id = \"SweatyCrayfish/llama-3-8b-quantized\"\n",
    "\n",
    "pipeline = transformers.pipeline(\"text-generation\", model=model_id, load_in_4bit=True, device_map=\"auto\",torch_dtype=torch.float32)\n",
    "pipeline(\"Hey how are you doing today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(\"SweatyCrayfish/llama-3-8b-quantized\", load_in_4bit=True, torch_dtype=torch.float32)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SweatyCrayfish/llama-3-8b-quantized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4bit.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "import torch \n",
    "#model_id = \"Meta-Llama-3-8B.Q2_K.gguf\"\n",
    "model_id = \"QuantFactory/Meta-Llama-3-8B-Instruct-GGUF-v2\"\n",
    "#model_4bit = AutoModelForCausalLM.from_pretrained(\"SweatyCrayfish/llama-3-8b-quantized\", load_in_4bit=True, torch_dtype=torch.float32)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"SweatyCrayfish/llama-3-8b-quantized\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"QuantFactory/Meta-Llama-3-8B-GGUF\")\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"QuantFactory/Meta-Llama-3-8B-Instruct-GGUF-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = torch.tensor(tokenizer.encode(\"Hey how are you doing today?\"))\n",
    "prompt = \"What is the Python programming language?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model_4bit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "\n",
    "out = F.softmax(y['logits'],dim=-1).argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(tokenizer)\n",
    "tokenizer.batch_decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer,pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"gpt2\", load_in_4bit=True, device_map=\"auto\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "#pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hello, how are you doing today?\"\n",
    "inputs = tokenizer(prompt,return_tensors=\"pt\").input_ids\n",
    "out = model.generate(inputs,max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out\n",
    "tokenizer.batch_decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"Hello, how are you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokenizer.default_chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the Python programming language?\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probs = F.softmax(logits['logits'],dim=-1)\n",
    "toks = probs.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.default_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", \"HuggingFaceH4/zephyr-7b-beta\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1])  # Print the assistant's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", \"HuggingFaceH4/zephyr-7b-beta\")\n",
    "pipe = pipeline(\"text-generation\",\"meta-llama/Llama-2-13b-chat-hf\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1])  # Print the assistant's response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modern_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
