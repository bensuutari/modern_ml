{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer,pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer,pipeline\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "class ChatBot(object):\n",
    "    def __init__(self,model_id):\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_id,load_in_4bit=True,)\n",
    "        self.model.eval()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        chat = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful and honest assistant.\"},\n",
    "        ]\n",
    "        print(tokenizer.apply_chat_template(chat, tokenize=False))\n",
    "    def chat(self,prompt):\n",
    "        #prompt = \"Hello, how are you doing today?\"\n",
    "        inputs = self.tokenizer(prompt,return_tensors=\"pt\").input_ids\n",
    "        with torch.no_grad():\n",
    "            out = self.model.generate(inputs,max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n",
    "        #probs = F.softmax(logits['logits'],dim=-1)\n",
    "        #toks = probs.argmax(dim=-1)\n",
    "        return self.tokenizer.batch_decode(out)#self.tokenizer.batch_decode(toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = ChatBot(\"meta-llama/Llama-2-13b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = chatbot.chat(\"Hello, please explain how a catalytic converter works.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = chatbot.tokenizer.batch_decode(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat pipeline\n",
    "\n",
    "Prompting with Llama2: https://discuss.huggingface.co/t/trying-to-understand-system-prompts-with-llama-2-and-transformers-interface/59016\n",
    "\n",
    "Pipelines in Huggingface: https://huggingface.co/docs/transformers/en/add_new_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,pipeline\n",
    "class ChatBot(object):\n",
    "    def __init__(self,model_id):\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id,load_in_4bit=True)\n",
    "        model.eval()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.chat_history = [\n",
    "            {\"role\": \"system\", \"content\" : \"You are a helpful and intelligent AI assistant who responds to user queries.\"}\n",
    "        ]\n",
    "        self.pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.num_tokens = len(self.tokenizer(self.chat_history[0]['content']).input_ids)\n",
    "        self.context_length = model.config.max_position_embeddings\n",
    "    def calculate_num_tokens(self,prompt):\n",
    "        self.num_tokens+=len(self.tokenizer(prompt).input_ids)\n",
    "        print('num_tokens:',self.num_tokens)\n",
    "    def chat(self,prompt):\n",
    "        self.calculate_num_tokens(prompt)\n",
    "        self.chat_history.append(\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        )\n",
    "        resp = self.pipe(self.chat_history,max_new_tokens=512)\n",
    "        print(resp)\n",
    "        self.chat_history.append(\n",
    "            {\"role\": \"system\", \"content\" : resp[0]['generated_text'][-1]['content']}\n",
    "        )\n",
    "        self.calculate_num_tokens(self.chat_history[-1][\"content\"])\n",
    "        return self.chat_history[-1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = ChatBot(\"meta-llama/Llama-2-13b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot.context_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chatbot.chat(\"What is a catalytic converter?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    response = chatbot.chat(\"Please formulate a smart question to your previous response.\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = chatbot.chat(\"Can you explain what you mean by 'catalyst'?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response3 = chatbot.chat(\"When was the catalytic converter invented?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot.num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response4 = chatbot.chat(\"What other parts of a car are important for pollution control?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot.calculate_num_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,load_in_4bit=True)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hey, can you tell me any fun things to do in New York?\"}\n",
    "]\n",
    "\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model,tokenizer=tokenizer, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "response = pipe(chat, max_new_tokens=512)\n",
    "print(response[0]['generated_text'][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(response)\n",
    "type(response[0])\n",
    "response[0].keys()\n",
    "response[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaotic playground:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "#model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model_id = \"SweatyCrayfish/llama-3-8b-quantized\"\n",
    "\n",
    "pipeline = transformers.pipeline(\"text-generation\", model=model_id, load_in_4bit=True, device_map=\"auto\",torch_dtype=torch.float32)\n",
    "pipeline(\"Hey how are you doing today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(\"SweatyCrayfish/llama-3-8b-quantized\", load_in_4bit=True, torch_dtype=torch.float32)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SweatyCrayfish/llama-3-8b-quantized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4bit.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "import torch \n",
    "#model_id = \"Meta-Llama-3-8B.Q2_K.gguf\"\n",
    "model_id = \"QuantFactory/Meta-Llama-3-8B-Instruct-GGUF-v2\"\n",
    "#model_4bit = AutoModelForCausalLM.from_pretrained(\"SweatyCrayfish/llama-3-8b-quantized\", load_in_4bit=True, torch_dtype=torch.float32)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"SweatyCrayfish/llama-3-8b-quantized\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"QuantFactory/Meta-Llama-3-8B-GGUF\")\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"QuantFactory/Meta-Llama-3-8B-Instruct-GGUF-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = torch.tensor(tokenizer.encode(\"Hey how are you doing today?\"))\n",
    "prompt = \"What is the Python programming language?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model_4bit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "\n",
    "out = F.softmax(y['logits'],dim=-1).argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(tokenizer)\n",
    "tokenizer.batch_decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer,pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"gpt2\", load_in_4bit=True, device_map=\"auto\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "#pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hello, how are you doing today?\"\n",
    "inputs = tokenizer(prompt,return_tensors=\"pt\").input_ids\n",
    "out = model.generate(inputs,max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out\n",
    "tokenizer.batch_decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"Hello, how are you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokenizer.default_chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the Python programming language?\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probs = F.softmax(logits['logits'],dim=-1)\n",
    "toks = probs.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.default_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", \"HuggingFaceH4/zephyr-7b-beta\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1])  # Print the assistant's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", \"HuggingFaceH4/zephyr-7b-beta\")\n",
    "pipe = pipeline(\"text-generation\",\"meta-llama/Llama-2-13b-chat-hf\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1])  # Print the assistant's response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modern_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
